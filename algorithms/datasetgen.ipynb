{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.areamanager as areamanager\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "import lib.cat_utils as cat_utils\n",
    "import lib.geo_utils as geo_utils\n",
    "from lib.constants import geocat_constants,experiment_constants\n",
    "from tqdm import tqdm_notebook\n",
    "import math\n",
    "\n",
    "SPLIT_YEAR=2017\n",
    "\n",
    "cities=['lasvegas','phoenix','charlotte','madison']\n",
    "#cities=['madison']\n",
    "\n",
    "dict_alias_title,category_tree,dict_alias_depth=cat_utils.cat_structs(\"../data/categories.json\")\n",
    "undirected_category_tree=category_tree.to_undirected()\n",
    "def category_filter(categories):\n",
    "    tmp_cat_list=list()\n",
    "    if categories != None:\n",
    "        for category in categories:\n",
    "            try:\n",
    "                if dict_alias_depth[dict_alias_title[category]] <= 2:\n",
    "                    tmp_cat_list.append(dict_alias_title[category])\n",
    "            except:\n",
    "                pass\n",
    "        tmp_cat_list=cat_utils.get_most_detailed_categories(tmp_cat_list,dict_alias_title,dict_alias_depth)\n",
    "    return tmp_cat_list\n",
    "\n",
    "\n",
    "TRAIN_SIZE=experiment_constants.TRAIN_SIZE\n",
    "TEST_SIZE=1-TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1859731674194336\n"
     ]
    }
   ],
   "source": [
    "fbusiness=open(\"../data/business.json\")\n",
    "poi_data = dict()\n",
    "start_time=time.time()\n",
    "for i, line in enumerate(fbusiness):  \n",
    "    # json to dict\n",
    "    obj_json = json.loads(line)\n",
    "    # add to the data collection\n",
    "    if obj_json['categories'] != None:\n",
    "        poi_data[obj_json['business_id']]={'latitude':obj_json['latitude'],\n",
    "                         'longitude':obj_json['longitude'],\n",
    "                         'categories':obj_json['categories'].split(', ')}\n",
    "    else:\n",
    "        poi_data[obj_json['business_id']]={'latitude':obj_json['latitude'],\n",
    "                 'longitude':obj_json['longitude'],\n",
    "                 'categories':obj_json['categories']}\n",
    "\n",
    "print(time.time()-start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area selected: Las Vegas\n",
      "Area selected: Phoenix\n",
      "Area selected: Charlotte\n",
      "Area selected: Madison\n"
     ]
    }
   ],
   "source": [
    "areas=dict()\n",
    "for city in cities:\n",
    "    areas[city]=areamanager.delimiter_area(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45136523246765137\n"
     ]
    }
   ],
   "source": [
    "cities_pid_in_area=dict()\n",
    "start_time=time.time()\n",
    "for city in cities:\n",
    "    area=areas[city]\n",
    "    pid_in_area=collections.defaultdict(bool)\n",
    "\n",
    "    for poi_id in poi_data:\n",
    "        if areamanager.poi_in_area(area,poi_data[poi_id]):\n",
    "\n",
    "            pid_in_area[poi_id]=True\n",
    "\n",
    "    cities_pid_in_area[city]=pid_in_area\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.028382301330566\n"
     ]
    }
   ],
   "source": [
    "fuser=open(\"../data/user.json\")\n",
    "user_data = dict()\n",
    "start_time=time.time()\n",
    "for i, line in enumerate(fuser):  \n",
    "    # json to dict\n",
    "    obj_json = json.loads(line)\n",
    "    # add to the data collection\n",
    "    user_data[obj_json['user_id']]=obj_json['friends'].split(', ')\n",
    "\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "3500000\n",
      "4000000\n",
      "4500000\n",
      "5000000\n",
      "5500000\n",
      "6000000\n",
      "6500000\n",
      "73.87379217147827\n",
      "0\n",
      "500000\n",
      "1000000\n",
      "8.45375919342041\n"
     ]
    }
   ],
   "source": [
    "freview=open(\"../data/review.json\")\n",
    "\n",
    "cities_checkin_data=dict()\n",
    "for city in cities:\n",
    "    cities_checkin_data[city]=list()\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "for i, line in enumerate(freview):  \n",
    "    # json to dict\n",
    "    obj_json = json.loads(line)\n",
    "    for city in cities:\n",
    "        if cities_pid_in_area[city][obj_json['business_id']]:\n",
    "            # add to the data collection\n",
    "            cities_checkin_data[city].append({'user_id':obj_json['user_id'],\n",
    "                             'poi_id':obj_json['business_id'],\n",
    "                             'date':obj_json['date']})\n",
    "            break\n",
    "    if i % 500000 ==0:\n",
    "        print(i)\n",
    "print(time.time()-start_time)\n",
    "\n",
    "ftip=open(\"../data/tip.json\")\n",
    "start_time=time.time()\n",
    "for i, line in enumerate(ftip):  \n",
    "    # json to dict\n",
    "    obj_json = json.loads(line)\n",
    "    for city in cities:\n",
    "        if cities_pid_in_area[city][obj_json['business_id']]:\n",
    "            # add to the data collection\n",
    "            cities_checkin_data[city].append({'user_id':obj_json['user_id'],\n",
    "                         'poi_id':obj_json['business_id'],\n",
    "                         'date':obj_json['date']})\n",
    "            break\n",
    "    if i % 500000 ==0:\n",
    "        print(i)\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_checkin=pd.read_csv(\"../data/checkin.csv\")\n",
    "\n",
    "# df_checkin=df_checkin.set_index(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city_area=areamanager.delimiter_area('madison')\n",
    "# df_checkin_city=areamanager.pois_in_area(city_area,df_checkin.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# for idx,checkin in df_checkin.iterrows():\n",
    "#    # print(checkin.business_id)\n",
    "#     if cities_pid_in_area['madison'][checkin.business_id]:\n",
    "#         i+=1\n",
    "\n",
    "# i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(df_checkin_city['business_id'].drop_duplicates()))\n",
    "# print(len(df_checkin_city['user_id'].drop_duplicates()))\n",
    "# print(len(df_checkin_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "genoptions=['poi','neighbor','user','checkin','test','train']\n",
    "genoptions=['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for city in cities:\n",
    "    print(\"CITY: %s\" % (city))\n",
    "    # Pega os checkins da cidade\n",
    "    checkin_data=cities_checkin_data[city]\n",
    "\n",
    "    # transforma em dataframe\n",
    "    df_checkin=pd.DataFrame.from_dict(checkin_data)\n",
    "    df_checkin.head(1)\n",
    "\n",
    "    # Começa a parte de filtragrem\n",
    "    df_diff_users_visited=df_checkin[['user_id','poi_id']].drop_duplicates().reset_index(drop=True).\\\n",
    "    groupby('poi_id').count().reset_index().rename(columns={\"user_id\":\"diffusersvisited\"})\n",
    "\n",
    "    df_diff_users_visited=df_diff_users_visited[df_diff_users_visited['diffusersvisited']>=5]\n",
    "\n",
    "    del df_diff_users_visited['diffusersvisited']\n",
    "    df_checkin=pd.merge(df_checkin,df_diff_users_visited,on='poi_id',how='inner')\n",
    "    df_checkin['Count']=df_checkin.groupby(['user_id'])['user_id'].transform('count')\n",
    "    df_checkin=df_checkin[df_checkin['Count']>=20]\n",
    "    del df_checkin['Count']\n",
    "    # converte para dicionario, ou lista de dicionarios\n",
    "    checkin_data=list(df_checkin.to_dict('index').values())\n",
    "\n",
    "    # termina a parte de filtragem\n",
    "    \n",
    "    # pega todos ids dos usuarios\n",
    "    users_id = set()\n",
    "    for check in checkin_data:\n",
    "        users_id.add(check['user_id'])\n",
    "    \n",
    "    # quantidade de usuarios\n",
    "    user_num=len(users_id)\n",
    "\n",
    "    # pega todos ids dos pois\n",
    "    pois_id = set()\n",
    "    for check in checkin_data:\n",
    "        pois_id.add(check['poi_id'])\n",
    "   \n",
    "    #quantidade de pois\n",
    "    poi_num=len(pois_id)\n",
    "    print(\"user_num:%d, poi_num:%d\"%(user_num,poi_num))\n",
    "\n",
    "    \n",
    "    # Começa a transformar ids de String para inteiro\n",
    "    users_id_to_int = dict()\n",
    "    for i,user_id in enumerate(users_id):\n",
    "        users_id_to_int[user_id]=i\n",
    "\n",
    "    pois_id_to_int = dict()\n",
    "    \n",
    "    for i,poi_id in enumerate(pois_id):\n",
    "        pois_id_to_int[poi_id]=i\n",
    "    # Termina de transformar ids de String para inteiro\n",
    "        \n",
    "    # cria dicionario de \"objetos\" ou dicionarios de pois da cidade\n",
    "    # alem de aplicar filtragem categorica\n",
    "    city_poi_data=dict()\n",
    "    if 'poi' in genoptions:\n",
    "        for poi_id in pois_id:\n",
    "            city_poi_data[pois_id_to_int[poi_id]]=poi_data[poi_id].copy()\n",
    "            city_poi_data[pois_id_to_int[poi_id]]['categories']=category_filter(poi_data[poi_id]['categories'])\n",
    "        fpoi=open('../data/poi/'+city+'.pickle','wb')\n",
    "        pickle.dump(city_poi_data,fpoi)\n",
    "        fpoi.close()\n",
    "    \n",
    "    \n",
    "    # pega os vizinhos de cada poi\n",
    "#     print(\"Pegando vizinhos...\")\n",
    "    if 'neighbor' in genoptions:\n",
    "        poi_neighbors={}\n",
    "        pois_id=[pois_id_to_int[pid] for pid in pois_id]\n",
    "        for i in tqdm_notebook(range(len(pois_id))):\n",
    "            poi_id = pois_id[i]\n",
    "            neighbors=list()\n",
    "            poi_neighbors[poi_id]=neighbors\n",
    "            for npoi_id in pois_id:\n",
    "                if geo_utils.haversine(city_poi_data[poi_id]['latitude'],city_poi_data[poi_id]['longitude'],\\\n",
    "                                      city_poi_data[npoi_id]['latitude'],city_poi_data[npoi_id]['longitude'])\\\n",
    "                <= geocat_constants.NEIGHBOR_DISTANCE:\n",
    "                    neighbors.append(npoi_id)\n",
    "        print(\"Terminou vizinhos...\")\n",
    "        fneighbors=open('../data/neighbor/'+city+'.pickle','wb')\n",
    "        pickle.dump(poi_neighbors,fneighbors)\n",
    "        fneighbors.close()\n",
    "    \n",
    "    city_user_data=dict()\n",
    "    countusf=0\n",
    "    print(\"Inicio Amigos...\")\n",
    "    users_id = list(users_id)\n",
    "    if 'user' in genoptions:\n",
    "        for i in tqdm_notebook(range(len(users_id))):\n",
    "            user_id=users_id[i]\n",
    "            ucity_friends=list()\n",
    "            for friend_id in user_data[user_id]:\n",
    "                try:\n",
    "                    ucity_friends.append(users_id_to_int[friend_id])\n",
    "                    countusf+=1\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            city_user_data[users_id_to_int[user_id]]=ucity_friends\n",
    "        fuser=open('../data/user/'+city+'.pickle','wb')\n",
    "        pickle.dump(city_user_data,fuser)\n",
    "        fuser.close()    \n",
    "    \n",
    "    print(\"Fim Amigos...\")\n",
    "    print(\"Friends: %d\"%(countusf))\n",
    "    if 'checkin' in genoptions:\n",
    "        for checkin in checkin_data:\n",
    "            checkin['user_id'] = users_id_to_int[checkin['user_id']]\n",
    "            checkin['poi_id'] = pois_id_to_int[checkin['poi_id']]\n",
    "            checkin['date'] = pd.to_datetime(checkin['date'])\n",
    "        fcheckin=open('../data/checkin/'+city+'.pickle','wb')\n",
    "        pickle.dump(checkin_data,fcheckin)\n",
    "        fcheckin.close()\n",
    "    #### Treino e teste por ano\n",
    "    # -==============================ANO=========================================.....\n",
    "#     df_test_checkin=pd.DataFrame(checkin_data)\n",
    "#     df_test_checkin=df_test_checkin[df_test_checkin.date>=pd.to_datetime(\"01/01/2017\")].reset_index(drop=True)\n",
    "#     #print(pd.DataFrame.from_dict(checkin))\n",
    "#     df_train_checkin=pd.DataFrame(checkin_data)\n",
    "#     df_train_checkin=df_train_checkin[df_train_checkin.date<pd.to_datetime(\"01/01/2017\")].reset_index(drop=True)\n",
    "    \n",
    "#     te_checkin_data=list(df_test_checkin.to_dict('index').values())\n",
    "#     tr_checkin_data=list(df_train_checkin.to_dict('index').values())\n",
    "    # -==============================ANO=========================================.....\n",
    "    \n",
    "    \n",
    "    #### Treino e teste com porcentagem\n",
    "    # -==============================PORCENTAGEM====================================================.....\n",
    "    tr_checkin_data=[]\n",
    "    te_checkin_data=[]\n",
    "    \n",
    "    user_checkin_data =dict()\n",
    "    for user_id in users_id:\n",
    "        user_checkin_data[users_id_to_int[user_id]]=list()\n",
    "    \n",
    "    for checkin in checkin_data:\n",
    "        user_checkin_data[checkin['user_id']].append({'poi_id':checkin['poi_id'],'date':checkin['date']})\n",
    "    \n",
    "    for i in tqdm_notebook(range(len(users_id))):\n",
    "        user_id=users_id_to_int[users_id[i]]\n",
    "        checkin_list=user_checkin_data[user_id]\n",
    "        checkin_list=sorted(checkin_list, key = lambda i: i['date']) \n",
    "        train_size=math.ceil(len(checkin_list)*TRAIN_SIZE)\n",
    "        #test_size=math.floor(len(checkin_list)*TEST_SIZE)\n",
    "        count=1\n",
    "        te_pois=set()\n",
    "        tr_pois=set()\n",
    "        initial_te_size=len(te_checkin_data)\n",
    "        final_te_size=len(te_checkin_data)\n",
    "        for checkin in checkin_list:\n",
    "            if count<=train_size:\n",
    "                tr_pois.add(checkin['poi_id'])\n",
    "                tr_checkin_data.append({'user_id':user_id,'poi_id':checkin['poi_id'],'date':checkin['date']})\n",
    "            else:\n",
    "                te_pois.add(checkin['poi_id'])\n",
    "                te_checkin_data.append({'user_id':user_id,'poi_id':checkin['poi_id'],'date':checkin['date']})\n",
    "                final_te_size+=1\n",
    "            count+=1\n",
    "        int_pois=te_pois&tr_pois\n",
    "        rel_index=0\n",
    "        for i in range(initial_te_size,final_te_size):\n",
    "            i+=rel_index\n",
    "            if te_checkin_data[i]['poi_id'] in int_pois:\n",
    "                te_checkin_data.pop(i)\n",
    "                rel_index-=1\n",
    "    # -==============================PORCENTAGEM====================================================.....\n",
    "    #### Treino e teste com porcentagem\n",
    "\n",
    "\n",
    "    ftecheckin=open('../data/checkin/test/'+city+'.pickle','wb')\n",
    "    pickle.dump(te_checkin_data,ftecheckin)\n",
    "    ftecheckin.close()\n",
    "    ftrcheckin=open('../data/checkin/train/'+city+'.pickle','wb')\n",
    "    pickle.dump(tr_checkin_data,ftrcheckin)\n",
    "    ftrcheckin.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('../data/user/madison.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charl=pickle.load(open('../data/user/charlotte.pickle','rb'))\n",
    "\n",
    "# a=0\n",
    "# for i in charl:\n",
    "    \n",
    "#     a+=len(charl[i])\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_checkin=pd.DataFrame.from_dict(checkin_data)\n",
    "# df_checkin.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(checkin_data)\n",
    "\n",
    "# users_id = set()\n",
    "# for check in checkin_data:\n",
    "#     users_id.add(check['user_id'])\n",
    "# #users_id=list(users_id)\n",
    "\n",
    "# user_num=len(users_id)\n",
    "# user_num\n",
    "\n",
    "# pois_id = set()\n",
    "# for check in checkin_data:\n",
    "#     pois_id.add(check['poi_id'])\n",
    "# #pois_id=list(pois_id)\n",
    "\n",
    "# poi_num=len(pois_id)\n",
    "# poi_num\n",
    "\n",
    "# users_id_to_int = dict()\n",
    "# for i,user_id in enumerate(users_id):\n",
    "#     users_id_to_int[user_id]=i\n",
    "\n",
    "# pois_id_to_int = dict()\n",
    "# for i,poi_id in enumerate(pois_id):\n",
    "#     pois_id_to_int[poi_id]=i\n",
    "\n",
    "# training_matrix = np.zeros((len(users_id),len(pois_id)))\n",
    "# for check in checkin_data:\n",
    "#     training_matrix[users_id_to_int[check['user_id']],pois_id_to_int[check['poi_id']]]+=1\n",
    "\n",
    "# diff_visits=np.count_nonzero(training_matrix,axis=0)\n",
    "\n",
    "# lids_subset=np.nonzero(diff_visits>=5)[0]\n",
    "\n",
    "# training_matrix=training_matrix[:,lids_subset]\n",
    "\n",
    "# users_visits=np.sum(training_matrix,axis=1)\n",
    "\n",
    "# uids_subset=np.nonzero(users_visits>=20)[0]\n",
    "\n",
    "# training_matrix=training_matrix[uids_subset,:]\n",
    "# np.sum(training_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
