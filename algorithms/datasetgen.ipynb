{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.areamanager as areamanager\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "import lib.geocat.cat_utils as cat_utils\n",
    "import lib.geo_utils as geo_utils\n",
    "from lib.constants import geocat_constants\n",
    "SPLIT_YEAR=2017\n",
    "\n",
    "#cities=['lasvegas','phoenix','charlotte','madison']\n",
    "cities=['madison']\n",
    "\n",
    "dict_alias_title,category_tree,dict_alias_depth=cat_utils.cat_structs()\n",
    "undirected_category_tree=category_tree.to_undirected()\n",
    "def category_filter(categories):\n",
    "    tmp_cat_list=list()\n",
    "    if categories != None:\n",
    "        for category in categories:\n",
    "            try:\n",
    "                if dict_alias_depth[dict_alias_title[category]] <= 2:\n",
    "                    tmp_cat_list.append(dict_alias_title[category])\n",
    "            except:\n",
    "                pass\n",
    "        tmp_cat_list=cat_utils.get_most_detailed_categories(tmp_cat_list,dict_alias_title,dict_alias_depth)\n",
    "    return tmp_cat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbusiness=open(\"../data/business.json\")\n",
    "poi_data = dict()\n",
    "start_time=time.time()\n",
    "for i, line in enumerate(fbusiness):  \n",
    "    # json to dict\n",
    "    obj_json = json.loads(line)\n",
    "    # add to the data collection\n",
    "    if obj_json['categories'] != None:\n",
    "        poi_data[obj_json['business_id']]={'latitude':obj_json['latitude'],\n",
    "                         'longitude':obj_json['longitude'],\n",
    "                         'categories':obj_json['categories'].split(', ')}\n",
    "    else:\n",
    "        poi_data[obj_json['business_id']]={'latitude':obj_json['latitude'],\n",
    "                 'longitude':obj_json['longitude'],\n",
    "                 'categories':obj_json['categories']}\n",
    "\n",
    "print(time.time()-start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas=dict()\n",
    "for city in cities:\n",
    "    areas[city]=areamanager.delimiter_area(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pid_in_area=dict()\n",
    "start_time=time.time()\n",
    "for city in cities:\n",
    "    area=areas[city]\n",
    "    pid_in_area=collections.defaultdict(bool)\n",
    "\n",
    "    for poi_id in poi_data:\n",
    "        if areamanager.poi_in_area(area,poi_data[poi_id]):\n",
    "\n",
    "            pid_in_area[poi_id]=True\n",
    "\n",
    "    cities_pid_in_area[city]=pid_in_area\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuser=open(\"../data/user.json\")\n",
    "user_data = dict()\n",
    "start_time=time.time()\n",
    "for i, line in enumerate(fuser):  \n",
    "    # json to dict\n",
    "    obj_json = json.loads(line)\n",
    "    # add to the data collection\n",
    "    user_data[obj_json['user_id']]=obj_json['friends'].split(', ')\n",
    "\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freview=open(\"../data/review.json\")\n",
    "\n",
    "cities_checkin_data=dict()\n",
    "for city in cities:\n",
    "    cities_checkin_data[city]=list()\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "for i, line in enumerate(freview):  \n",
    "    # json to dict\n",
    "    obj_json = json.loads(line)\n",
    "    for city in cities:\n",
    "        if cities_pid_in_area[city][obj_json['business_id']]:\n",
    "            # add to the data collection\n",
    "            cities_checkin_data[city].append({'user_id':obj_json['user_id'],\n",
    "                             'poi_id':obj_json['business_id'],\n",
    "                             'date':obj_json['date']})\n",
    "            break\n",
    "    if i % 500000 ==0:\n",
    "        print(i)\n",
    "print(time.time()-start_time)\n",
    "\n",
    "ftip=open(\"../data/tip.json\")\n",
    "start_time=time.time()\n",
    "for i, line in enumerate(ftip):  \n",
    "    # json to dict\n",
    "    obj_json = json.loads(line)\n",
    "    for city in cities:\n",
    "        if cities_pid_in_area[city][obj_json['business_id']]:\n",
    "            # add to the data collection\n",
    "            cities_checkin_data[city].append({'user_id':obj_json['user_id'],\n",
    "                         'poi_id':obj_json['business_id'],\n",
    "                         'date':obj_json['date']})\n",
    "            break\n",
    "    if i % 500000 ==0:\n",
    "        print(i)\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_checkin=pd.read_csv(\"../data/checkin.csv\")\n",
    "\n",
    "# df_checkin=df_checkin.set_index(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city_area=areamanager.delimiter_area('madison')\n",
    "# df_checkin_city=areamanager.pois_in_area(city_area,df_checkin.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# for idx,checkin in df_checkin.iterrows():\n",
    "#    # print(checkin.business_id)\n",
    "#     if cities_pid_in_area['madison'][checkin.business_id]:\n",
    "#         i+=1\n",
    "\n",
    "# i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(df_checkin_city['business_id'].drop_duplicates()))\n",
    "# print(len(df_checkin_city['user_id'].drop_duplicates()))\n",
    "# print(len(df_checkin_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_poi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for city in cities:\n",
    "    print(\"CITY: %s\" % (city))\n",
    "    # Pega os checkins da cidade\n",
    "    checkin_data=cities_checkin_data[city]\n",
    "\n",
    "    # transofrma em dataframe\n",
    "    df_checkin=pd.DataFrame.from_dict(checkin_data)\n",
    "    df_checkin.head(1)\n",
    "\n",
    "    # Começa a parte de filtagrem\n",
    "    df_diff_users_visited=df_checkin[['user_id','poi_id']].drop_duplicates().reset_index(drop=True).\\\n",
    "    groupby('poi_id').count().reset_index().rename(columns={\"user_id\":\"diffusersvisited\"})\n",
    "\n",
    "    df_diff_users_visited=df_diff_users_visited[df_diff_users_visited['diffusersvisited']>=5]\n",
    "\n",
    "    del df_diff_users_visited['diffusersvisited']\n",
    "    df_checkin=pd.merge(df_checkin,df_diff_users_visited,on='poi_id',how='inner')\n",
    "    df_checkin['Count']=df_checkin.groupby(['user_id'])['user_id'].transform('count')\n",
    "    df_checkin=df_checkin[df_checkin['Count']>=20]\n",
    "    del df_checkin['Count']\n",
    "    # converte para dicionario, ou lista de dicionarios\n",
    "    checkin_data=list(df_checkin.to_dict('index').values())\n",
    "    # termina a parte de filtragem\n",
    "    \n",
    "    # pega todos ids dos usuarios\n",
    "    users_id = set()\n",
    "    for check in checkin_data:\n",
    "        users_id.add(check['user_id'])\n",
    "    \n",
    "    # quantidade de usuarios\n",
    "    user_num=len(users_id)\n",
    "\n",
    "    # pega todos ids dos pois\n",
    "    pois_id = set()\n",
    "    for check in checkin_data:\n",
    "        pois_id.add(check['poi_id'])\n",
    "   \n",
    "    #quantidade de pois\n",
    "    poi_num=len(pois_id)\n",
    "    print(\"user_num:%d, poi_num:%d\"%(user_num,poi_num))\n",
    "\n",
    "    \n",
    "    # Começa a transformar ids de String para inteiro\n",
    "    users_id_to_int = dict()\n",
    "    for i,user_id in enumerate(users_id):\n",
    "        users_id_to_int[user_id]=i\n",
    "\n",
    "    pois_id_to_int = dict()\n",
    "    \n",
    "    for i,poi_id in enumerate(pois_id):\n",
    "        pois_id_to_int[poi_id]=i\n",
    "    # Termina de transformar ids de String para inteiro\n",
    "        \n",
    "    # cria dicionario de \"objetos\" ou dicionarios de pois da cidade\n",
    "    # alem de aplicar filtragem categorica\n",
    "    city_poi_data=dict()\n",
    "    \n",
    "    for poi_id in pois_id:\n",
    "        city_poi_data[pois_id_to_int[poi_id]]=poi_data[poi_id]\n",
    "        city_poi_data[pois_id_to_int[poi_id]]['categories']=category_filter(city_poi_data[pois_id_to_int[poi_id]]['categories'])\n",
    "    # pega os vizinhos de cada poi\n",
    "    print(\"Pegando vizinhos...\")\n",
    "    poi_neighbors={}\n",
    "    pois_id=[pois_id_to_int[pid] for pid in pois_id]\n",
    "    for poi_id in pois_id:    \n",
    "        neighbors=list()\n",
    "        poi_neighbors[poi_id]=neighbors\n",
    "        for npoi_id in pois_id:\n",
    "            if geo_utils.haversine(city_poi_data[poi_id]['latitude'],city_poi_data[poi_id]['longitude'],\\\n",
    "                                  city_poi_data[npoi_id]['latitude'],city_poi_data[npoi_id]['longitude'])\\\n",
    "            <= geocat_constants.get_neighbor_distance():\n",
    "                neighbors.append(npoi_id)\n",
    "    print(\"Terminou vizinhos...\")\n",
    "    city_user_data=dict()\n",
    "    countusf=0\n",
    "    print(\"Inicio Amigos...\")\n",
    "    for i in users_id:\n",
    "        ucity_friends=list()\n",
    "        try:\n",
    "            for friend_id in user_data[i]:\n",
    "                ucity_friends.append(users_id_to_int[friend_id])\n",
    "                countusf+=1\n",
    "        except:\n",
    "            pass\n",
    "        city_user_data[users_id_to_int[i]]=ucity_friends\n",
    "    print(\"Fim Amigos...\")\n",
    "    print(\"Friends: %d\"%(countusf))\n",
    "    for checkin in checkin_data:\n",
    "        checkin['user_id'] = users_id_to_int[checkin['user_id']]\n",
    "        checkin['poi_id'] = pois_id_to_int[checkin['poi_id']]\n",
    "        checkin['date'] = pd.to_datetime(checkin['date'])\n",
    "    \n",
    "    df_test_checkin=pd.DataFrame(checkin_data)\n",
    "    df_test_checkin=df_test_checkin[df_test_checkin.date>=pd.to_datetime(\"01/01/2017\")].reset_index(drop=True)\n",
    "    #print(pd.DataFrame.from_dict(checkin))\n",
    "    df_train_checkin=pd.DataFrame(checkin_data)\n",
    "    df_train_checkin=df_train_checkin[df_train_checkin.date<pd.to_datetime(\"01/01/2017\")].reset_index(drop=True)\n",
    "    \n",
    "    te_checkin_data=list(df_test_checkin.to_dict('index').values())\n",
    "    tr_checkin_data=list(df_train_checkin.to_dict('index').values())\n",
    "    \n",
    "\n",
    "    fcheckin=open('../data/checkin/'+city+'.pickle','wb')\n",
    "    fpoi=open('../data/poi/'+city+'.pickle','wb')\n",
    "    fuser=open('../data/user/'+city+'.pickle','wb')\n",
    "    ftrcheckin=open('../data/checkin/train/'+city+'.pickle','wb')\n",
    "    ftecheckin=open('../data/checkin/test/'+city+'.pickle','wb')\n",
    "    \n",
    "    pickle.dump(checkin_data,fcheckin)\n",
    "    fcheckin.close()\n",
    "    pickle.dump(city_poi_data,fpoi)\n",
    "    fpoi.close()\n",
    "    pickle.dump(city_user_data,fuser)\n",
    "    fuser.close()\n",
    "    pickle.dump(tr_checkin_data,ftrcheckin)\n",
    "    ftrcheckin.close()\n",
    "    pickle.dump(te_checkin_data,ftecheckin)\n",
    "    ftecheckin.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('../data/user/madison.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charl=pickle.load(open('../data/user/charlotte.pickle','rb'))\n",
    "\n",
    "# a=0\n",
    "# for i in charl:\n",
    "    \n",
    "#     a+=len(charl[i])\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_checkin=pd.DataFrame.from_dict(checkin_data)\n",
    "# df_checkin.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(checkin_data)\n",
    "\n",
    "# users_id = set()\n",
    "# for check in checkin_data:\n",
    "#     users_id.add(check['user_id'])\n",
    "# #users_id=list(users_id)\n",
    "\n",
    "# user_num=len(users_id)\n",
    "# user_num\n",
    "\n",
    "# pois_id = set()\n",
    "# for check in checkin_data:\n",
    "#     pois_id.add(check['poi_id'])\n",
    "# #pois_id=list(pois_id)\n",
    "\n",
    "# poi_num=len(pois_id)\n",
    "# poi_num\n",
    "\n",
    "# users_id_to_int = dict()\n",
    "# for i,user_id in enumerate(users_id):\n",
    "#     users_id_to_int[user_id]=i\n",
    "\n",
    "# pois_id_to_int = dict()\n",
    "# for i,poi_id in enumerate(pois_id):\n",
    "#     pois_id_to_int[poi_id]=i\n",
    "\n",
    "# training_matrix = np.zeros((len(users_id),len(pois_id)))\n",
    "# for check in checkin_data:\n",
    "#     training_matrix[users_id_to_int[check['user_id']],pois_id_to_int[check['poi_id']]]+=1\n",
    "\n",
    "# diff_visits=np.count_nonzero(training_matrix,axis=0)\n",
    "\n",
    "# lids_subset=np.nonzero(diff_visits>=5)[0]\n",
    "\n",
    "# training_matrix=training_matrix[:,lids_subset]\n",
    "\n",
    "# users_visits=np.sum(training_matrix,axis=1)\n",
    "\n",
    "# uids_subset=np.nonzero(users_visits>=20)[0]\n",
    "\n",
    "# training_matrix=training_matrix[uids_subset,:]\n",
    "# np.sum(training_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
